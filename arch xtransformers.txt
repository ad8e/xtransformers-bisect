

stories 2732634
chars 2191281193
longest 4433
chars used: 52428800
dataset proportion used: 0.023926094089376872
74 chars used:
 !"$',-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
TransformerWrapper                                 [1, 1024, 75]             --
├─TokenEmbedding: 1-1                              [1, 1024, 384]            --
│    └─Embedding: 2-1                              [1, 1024, 384]            29,184
├─Identity: 1-2                                    [1, 1024, 384]            --
├─Dropout: 1-3                                     [1, 1024, 384]            --
├─Identity: 1-4                                    [1, 1024, 384]            --
├─Decoder: 1-5                                     [1, 1024, 384]            --
│    └─ModuleList: 2-2                             --                        --
│    │    └─ModuleList: 3-1                        --                        --
│    │    │    └─ModuleList: 4-1                   --                        --
│    │    │    │    └─LayerNorm: 5-1               [1, 1024, 384]            --
│    │    │    └─Attention: 4-2                    [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-2                  [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-3                  [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-4                  [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-5                  [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-6                  [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-3                     [1, 1024, 384]            --
│    │    └─ModuleList: 3-2                        --                        --
│    │    │    └─ModuleList: 4-4                   --                        --
│    │    │    │    └─LayerNorm: 5-7               [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-5                  [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-8              [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-1         [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-1        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-2          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-2            [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-3             [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-6                     [1, 1024, 384]            --
│    │    └─ModuleList: 3-3                        --                        --
│    │    │    └─ModuleList: 4-7                   --                        --
│    │    │    │    └─LayerNorm: 5-9               [1, 1024, 384]            --
│    │    │    └─Attention: 4-8                    [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-10                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-11                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-12                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-13                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-14                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-9                     [1, 1024, 384]            --
│    │    └─ModuleList: 3-4                        --                        --
│    │    │    └─ModuleList: 4-10                  --                        --
│    │    │    │    └─LayerNorm: 5-15              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-11                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-16             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-4         [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-3        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-4          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-5            [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-6             [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-12                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-5                        --                        --
│    │    │    └─ModuleList: 4-13                  --                        --
│    │    │    │    └─LayerNorm: 5-17              [1, 1024, 384]            --
│    │    │    └─Attention: 4-14                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-18                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-19                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-20                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-21                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-22                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-15                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-6                        --                        --
│    │    │    └─ModuleList: 4-16                  --                        --
│    │    │    │    └─LayerNorm: 5-23              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-17                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-24             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-7         [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-5        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-6          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-8            [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-9             [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-18                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-7                        --                        --
│    │    │    └─ModuleList: 4-19                  --                        --
│    │    │    │    └─LayerNorm: 5-25              [1, 1024, 384]            --
│    │    │    └─Attention: 4-20                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-26                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-27                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-28                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-29                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-30                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-21                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-8                        --                        --
│    │    │    └─ModuleList: 4-22                  --                        --
│    │    │    │    └─LayerNorm: 5-31              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-23                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-32             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-10        [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-7        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-8          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-11           [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-12            [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-24                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-9                        --                        --
│    │    │    └─ModuleList: 4-25                  --                        --
│    │    │    │    └─LayerNorm: 5-33              [1, 1024, 384]            --
│    │    │    └─Attention: 4-26                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-34                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-35                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-36                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-37                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-38                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-27                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-10                       --                        --
│    │    │    └─ModuleList: 4-28                  --                        --
│    │    │    │    └─LayerNorm: 5-39              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-29                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-40             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-13        [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-9        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-10         [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-14           [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-15            [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-30                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-11                       --                        --
│    │    │    └─ModuleList: 4-31                  --                        --
│    │    │    │    └─LayerNorm: 5-41              [1, 1024, 384]            --
│    │    │    └─Attention: 4-32                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-42                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-43                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-44                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-45                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-46                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-33                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-12                       --                        --
│    │    │    └─ModuleList: 4-34                  --                        --
│    │    │    │    └─LayerNorm: 5-47              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-35                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-48             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-16        [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-11       [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-12         [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-17           [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-18            [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-36                    [1, 1024, 384]            --
│    └─LayerNorm: 2-3                              [1, 1024, 384]            --
├─Linear: 1-6                                      [1, 1024, 75]             28,800
====================================================================================================
Total params: 10,674,816
Trainable params: 10,674,816
Non-trainable params: 0
Total mult-adds (M): 10.67
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 173.63
Params size (MB): 42.70
Estimated Total Size (MB): 216.34
====================================================================================================
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
TransformerWrapper                                 [1, 1024, 75]             --
├─TokenEmbedding: 1-1                              [1, 1024, 384]            --
│    └─Embedding: 2-1                              [1, 1024, 384]            29,184
├─Identity: 1-2                                    [1, 1024, 384]            --
├─Dropout: 1-3                                     [1, 1024, 384]            --
├─Identity: 1-4                                    [1, 1024, 384]            --
├─Decoder: 1-5                                     [1, 1024, 384]            --
│    └─ModuleList: 2-2                             --                        --
│    │    └─ModuleList: 3-1                        --                        --
│    │    │    └─ModuleList: 4-1                   --                        --
│    │    │    │    └─LayerNorm: 5-1               [1, 1024, 384]            --
│    │    │    └─Attention: 4-2                    [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-2                  [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-3                  [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-4                  [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-5                  [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-6                  [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-3                     [1, 1024, 384]            --
│    │    └─ModuleList: 3-2                        --                        --
│    │    │    └─ModuleList: 4-4                   --                        --
│    │    │    │    └─LayerNorm: 5-7               [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-5                  [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-8              [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-1         [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-1        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-2          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-2            [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-3             [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-6                     [1, 1024, 384]            --
│    │    └─ModuleList: 3-3                        --                        --
│    │    │    └─ModuleList: 4-7                   --                        --
│    │    │    │    └─LayerNorm: 5-9               [1, 1024, 384]            --
│    │    │    └─Attention: 4-8                    [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-10                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-11                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-12                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-13                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-14                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-9                     [1, 1024, 384]            --
│    │    └─ModuleList: 3-4                        --                        --
│    │    │    └─ModuleList: 4-10                  --                        --
│    │    │    │    └─LayerNorm: 5-15              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-11                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-16             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-4         [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-3        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-4          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-5            [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-6             [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-12                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-5                        --                        --
│    │    │    └─ModuleList: 4-13                  --                        --
│    │    │    │    └─LayerNorm: 5-17              [1, 1024, 384]            --
│    │    │    └─Attention: 4-14                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-18                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-19                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-20                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-21                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-22                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-15                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-6                        --                        --
│    │    │    └─ModuleList: 4-16                  --                        --
│    │    │    │    └─LayerNorm: 5-23              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-17                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-24             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-7         [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-5        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-6          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-8            [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-9             [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-18                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-7                        --                        --
│    │    │    └─ModuleList: 4-19                  --                        --
│    │    │    │    └─LayerNorm: 5-25              [1, 1024, 384]            --
│    │    │    └─Attention: 4-20                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-26                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-27                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-28                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-29                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-30                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-21                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-8                        --                        --
│    │    │    └─ModuleList: 4-22                  --                        --
│    │    │    │    └─LayerNorm: 5-31              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-23                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-32             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-10        [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-7        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-8          [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-11           [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-12            [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-24                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-9                        --                        --
│    │    │    └─ModuleList: 4-25                  --                        --
│    │    │    │    └─LayerNorm: 5-33              [1, 1024, 384]            --
│    │    │    └─Attention: 4-26                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-34                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-35                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-36                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-37                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-38                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-27                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-10                       --                        --
│    │    │    └─ModuleList: 4-28                  --                        --
│    │    │    │    └─LayerNorm: 5-39              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-29                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-40             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-13        [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-9        [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-10         [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-14           [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-15            [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-30                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-11                       --                        --
│    │    │    └─ModuleList: 4-31                  --                        --
│    │    │    │    └─LayerNorm: 5-41              [1, 1024, 384]            --
│    │    │    └─Attention: 4-32                   [1, 1024, 384]            --
│    │    │    │    └─Linear: 5-42                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-43                 [1, 1024, 384]            147,456
│    │    │    │    └─Linear: 5-44                 [1, 1024, 384]            147,456
│    │    │    │    └─Attend: 5-45                 [1, 6, 1024, 64]          --
│    │    │    │    └─Linear: 5-46                 [1, 1024, 384]            147,456
│    │    │    └─Residual: 4-33                    [1, 1024, 384]            --
│    │    └─ModuleList: 3-12                       --                        --
│    │    │    └─ModuleList: 4-34                  --                        --
│    │    │    │    └─LayerNorm: 5-47              [1, 1024, 384]            --
│    │    │    └─FeedForward: 4-35                 [1, 1024, 384]            --
│    │    │    │    └─Sequential: 5-48             [1, 1024, 384]            --
│    │    │    │    │    └─Sequential: 6-16        [1, 1024, 1536]           --
│    │    │    │    │    │    └─Linear: 7-11       [1, 1024, 1536]           589,824
│    │    │    │    │    │    └─GELU: 7-12         [1, 1024, 1536]           --
│    │    │    │    │    └─Dropout: 6-17           [1, 1024, 1536]           --
│    │    │    │    │    └─Linear: 6-18            [1, 1024, 384]            589,824
│    │    │    └─Residual: 4-36                    [1, 1024, 384]            --
│    └─LayerNorm: 2-3                              [1, 1024, 384]            --
├─Linear: 1-6                                      [1, 1024, 75]             28,800
====================================================================================================
Total params: 10,674,816
Trainable params: 10,674,816
Non-trainable params: 0
Total mult-adds (M): 10.67
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 173.63
Params size (MB): 42.70
Estimated Total Size (MB): 216.34
====================================================================================================

wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: ad8e (team-ad8e). Use `wandb login --relogin` to force relogin

Tracking run with wandb version 0.16.1
Run data is saved locally in /content/drive/MyDrive/tinystories/wandb/run-20240105_233911-qorimmsn
Syncing run x-transformers to Weights & Biases (docs)
View project at https://wandb.ai/team-ad8e/tinystories2
View run at https://wandb.ai/team-ad8e/tinystories2/runs/qorimmsn

net.token_emb.emb.weight [0.07201846688985825, -0.0008242803742177784]
net.attn_layers.layers.0.1.to_q.weight [0.02947484701871872, -9.197422332363203e-05]
net.attn_layers.layers.0.1.to_k.weight [0.029471317306160927, -1.2622667782125063e-06]
net.attn_layers.layers.0.1.to_v.weight [0.02943297289311886, 0.00011848389840452]
net.attn_layers.layers.0.1.to_out.weight [0.029447240754961967, 0.00011311377602396533]
net.attn_layers.layers.1.1.ff.0.0.weight [0.029481835663318634, 6.774022767785937e-05]
net.attn_layers.layers.1.1.ff.2.weight [0.014717836864292622, 1.189221438835375e-05]
net.attn_layers.layers.2.1.to_q.weight [0.02938736416399479, -0.00016918114852160215]
net.attn_layers.layers.2.1.to_k.weight [0.02949194610118866, 3.463836037553847e-05]
net.attn_layers.layers.2.1.to_v.weight [0.029496243223547935, -2.641788887558505e-05]
net.attn_layers.layers.2.1.to_out.weight [0.02943642996251583, -3.414197635720484e-05]
net.attn_layers.layers.3.1.ff.0.0.weight [0.029451264068484306, 1.633859756111633e-05]
net.attn_layers.layers.3.1.ff.2.weight [0.014733750373125076, 2.1515219486900605e-05]
net.attn_layers.layers.4.1.to_q.weight [0.029496923089027405, 2.9938108127680607e-05]
net.attn_layers.layers.4.1.to_k.weight [0.029474250972270966, -5.201056046644226e-05]
net.attn_layers.layers.4.1.to_v.weight [0.02950267307460308, -2.7118641810375266e-05]
net.attn_layers.layers.4.1.to_out.weight [0.029448404908180237, -6.376999226631597e-05]
net.attn_layers.layers.5.1.ff.0.0.weight [0.029460305348038673, -1.5887419067439623e-05]
net.attn_layers.layers.5.1.ff.2.weight [0.014729768969118595, 1.5030937902338337e-05]
net.attn_layers.layers.6.1.to_q.weight [0.029415398836135864, 0.0001254994422197342]
net.attn_layers.layers.6.1.to_k.weight [0.029438111931085587, 2.6519513994571753e-05]
net.attn_layers.layers.6.1.to_v.weight [0.02941780537366867, 1.8944369003293104e-05]
net.attn_layers.layers.6.1.to_out.weight [0.029462331905961037, -3.574668153305538e-05]
net.attn_layers.layers.7.1.ff.0.0.weight [0.029452966526150703, -2.525246236473322e-05]
net.attn_layers.layers.7.1.ff.2.weight [0.014736631885170937, 2.42910073211533e-06]
net.attn_layers.layers.8.1.to_q.weight [0.029464716091752052, 6.813303343733423e-07]
net.attn_layers.layers.8.1.to_k.weight [0.02943321131169796, -2.5359202027175343e-06]
net.attn_layers.layers.8.1.to_v.weight [0.02948836423456669, -0.00010392870171926916]
net.attn_layers.layers.8.1.to_out.weight [0.029440822079777718, -5.9595600760076195e-05]
net.attn_layers.layers.9.1.ff.0.0.weight [0.029484540224075317, -4.6024651965126395e-05]
net.attn_layers.layers.9.1.ff.2.weight [0.01473118644207716, -1.825926301535219e-05]
net.attn_layers.layers.10.1.to_q.weight [0.029493506997823715, -3.187997208442539e-05]
net.attn_layers.layers.10.1.to_k.weight [0.029438834637403488, -0.00010270137863699347]
net.attn_layers.layers.10.1.to_v.weight [0.029457826167345047, -8.53322126204148e-06]
net.attn_layers.layers.10.1.to_out.weight [0.0294493380934, -3.0361816243384965e-06]
net.attn_layers.layers.11.1.ff.0.0.weight [0.029458124190568924, -1.3505130027624546e-06]
net.attn_layers.layers.11.1.ff.2.weight [0.01473216712474823, 1.2893008715764154e-05]
net.to_logits.weight [0.029470644891262054, 7.185207505244762e-05]
WNnTnTnpTnn n N kNpnskpnr Tka nknsnsTpTT napAT rnk pnspnpTrspTnk p Nnnkrrpnnrs   n srNsNnns  nkkNTnTNrsknpssTsnpArnrpNksppTknsrNNnrrpsTrTT snTNpAsp rN nrTnrpsNrs trrkn ssNN  t rnpnaNskk tskpn nanTrkNrpnssnrpnnkNTnnknnnrTnrrNnssnppppTnapnNnnNTrnrnspnrpTpsskN NkNknrpTsNksk s rrs  s rrAsTnNskaNppAasnpNrrrNp rNrnaNnrn  naspssTAssNrnsrTn  rAANns kskrsTTrnspnnsNksksnnrpTrNsNrp sNkNpNrArpNn rnasrApsTn ps knrrTA sp TTspAppSTpnnrAaaanrrAnpsapTAs pnkppNTAtsrAassnrnpsnrpS rppsTTnrAppnnkrsnarT nsNspnksps tNkps rNrnassNTNsTpNNr00npnapn rAr Nnprn n npnstrAspTApAaTnprnpsNssprpApn NpprnsNsNkr psssN NknnaT kassNrNsrNpNsTnnkptpsrpTnknNspTptNkkts rNsnrpsrAaanpNN tsrprrrnrANpsrAnNkksnrnrAnnn kpAtNs TtNNTtApspNnkkprArr rsrnrNktstsrNrNr0pNnstWrAtrANpsprsspTTrpsnkpAsprpssTr NNssppnkrrNs t spTnrr0ns pAanapntrpNApnpTAppns kkpnar0sNrtANs knNrAnsNAtntnaArspApNpANAsDNNNnapnNnarsnrpss NkspnnprnspsDNr00tnkprprNpNptpnaarrnrrnpptnrr00ssAtWrpTtDrpstWspNApTAssNkstnNNnNnN!sNspsAanrrsnnrr00NprsnpApspnrnpsrnANnNnNNnrrN!tAsApnsDWstpppNpnrrppAaAAa
getting in the ground, the leadny friend in his funny full thing and see it was there!@After a was a big clery bear was all, and said, "Lee is maging, tree!"
And Sally was helpful delived to see the day flying and had a lot. Everyone smiled.".
From that day on, the tree to make more to how her hoot in the proper. Sallyon the street care and happy.
The lot of the cateriand hugged the lools. He saon,depensive and thanks. Fie's that day. Eation, the tree was still friends thank the tree together. Elly and the sky became brip would blid and they can had their bright friends.
And they can leat their back. No knew their mom. They store careled around the befforeabble to go hold many look. From then did with lots of brigg and played together.@
There learned that it always rain,
Amy who cold bunning again. They are so happy in the food.@Bell happy, they are had bad. They both lived at around them all bubar outs in highs traper.@John's mommy was spoiled with a spaces in the fruit.@lots in his things.@And of the soun
Once there was a bit boy who learned a lot of gears who always loved playing with her friends. The end of the day they learned their lesson that day.
The end potaches int a bug and give thim. Finally, Sally the boy had taucht the windest day things away. Sally liked that it was too clever the windestant.
"Well you find my day try," said Sally. "You are allow this take your brother try. Would you like to show me," said Sally.
The end from learned that even when the boy wasn't frustrated how to listen that?"
The boy was happy with Sally, and Sally would like all her friends. They'd became back and happily. They learned that it is important to know what thinks away too.
The end. They shared and lots of fun for, but schonesting weekline. They all learned that fruit would show their friendship could all showsteris, but now we can listen to her friends. They were happy!@The end The end.@They arrive and the boy were the best of fun to eat. They worked and had a fun playing and eating their sacks. Suddenly, there
Once upon a time there was a baby castly. She loved to run and joke. One day, she was too happy. He wanted to give her a special prize. Sally was best friend after all, her friends, and they all liked her own jokes.
The moral of the story is that even if teach you help off, you need some of the razor with some friends on that will stop the butter castling friendship is an old of here. When you get your brother's razor, water that tweeties is near the razor, water insteads. Would you're the other old of your brother and not to remain with shop. But there he wouldn't stay clean. The little old one is important friend to help others. There are one was scared and told her to be afraid.
Suddenly, the old one had saved her friendship and ran to the old one animal and went home. She was a vowed castle, like she had such more friendship and specially with it.@And they allowed the ones together with the old ones will to eat.@They were three years old, and other animals. They were very hoppper and cozy. The old ones we
Once upon a time, there was a green. Her brother.@Once upon a time, there was a stubborn gray dog, and to her new chair. She had an adventure. Every day, she and her mom would try a big truck and she had learned an important lesson. They all lived happily ever after in the beautiful stratch. The end with a wash, they both had fixed their chair.
The moral of the story is to always ask for help after what doing we do, and you never know what will happen if you deserve what it is? So it collects our friend is delicious! If you want it, you can have a brand day when you don't get it. That is all it?'
The moral of this story is that it is important to ask for hours, asking as the stubborn is important. Sometimes, even if you do a garage important lesson, it will happen if you don't have to support each other out as our own, it's always.@Once upon a time, there was a little boy named Tom. The boy was happy.@Once upon a time, in a big, pretty pond, they were an important pole.@Once upon a time, there was a little m
Mummy and Sally were best.@Once upon a time, they would grown. Everyday, Sally, Jall was feeling ready to go to admire the razor too.
One day, the two friends were also fearful to admire the razor that she summer. They were a big green surprise! They hugged Sally and Sally. They laughed and watched their brothers that had brought a fraz story and wonderful treats. They had so much fun that it was so magical before they went back to saving the same treats to everyone.
But when they got to the fence, the best friend was happy and they checked up their treats. The best tree was growing best friends, and it couldn't wait to save the fence to his brother and put the huge beautiful treats for their treats. The two frazors was so happy to be able to clean this until he walked out of the surprise, they spread a bad ending to play.@The sun watched out for a supplies. Every day he spent the supplies.@Bird was very frustrated with joy and worked. One day, a nurse day, trying to be safely.@John and Lucy are good twins.
net.token_emb.emb.weight [0.08686325699090958, -0.0009426155593246222]
net.attn_layers.layers.0.1.to_q.weight [0.027640197426080704, -8.784859528532252e-05]
net.attn_layers.layers.0.1.to_k.weight [0.028424685820937157, -6.4155810832744464e-06]
net.attn_layers.layers.0.1.to_v.weight [0.026053953915834427, 8.903423440642655e-05]
net.attn_layers.layers.0.1.to_out.weight [0.024869568645954132, 0.00010222224955214188]
net.attn_layers.layers.1.1.ff.0.0.weight [0.037761058658361435, 2.1197483874857426e-05]
net.attn_layers.layers.1.1.ff.2.weight [0.02735263481736183, 3.6142810131423175e-05]
net.attn_layers.layers.2.1.to_q.weight [0.0336192287504673, -0.00011788128176704049]
net.attn_layers.layers.2.1.to_k.weight [0.0334828682243824, 7.082956290105358e-05]
net.attn_layers.layers.2.1.to_v.weight [0.03419630974531174, -2.6819276172318496e-05]
net.attn_layers.layers.2.1.to_out.weight [0.036129120737314224, -2.6558487661532126e-05]
net.attn_layers.layers.3.1.ff.0.0.weight [0.045840974897146225, 4.284644455765374e-05]
net.attn_layers.layers.3.1.ff.2.weight [0.0343390554189682, 8.153717317327391e-06]
net.attn_layers.layers.4.1.to_q.weight [0.03694355487823486, 3.1384246540255845e-05]
net.attn_layers.layers.4.1.to_k.weight [0.03611287102103233, -5.0894497690023854e-05]
net.attn_layers.layers.4.1.to_v.weight [0.03727186471223831, 6.682018010906177e-06]
net.attn_layers.layers.4.1.to_out.weight [0.03865965083241463, -3.98643605876714e-05]
net.attn_layers.layers.5.1.ff.0.0.weight [0.045164380222558975, -2.2708589312969707e-05]
net.attn_layers.layers.5.1.ff.2.weight [0.03411576524376869, 1.017709837469738e-05]
net.attn_layers.layers.6.1.to_q.weight [0.038120072335004807, 0.00011343445657985285]
net.attn_layers.layers.6.1.to_k.weight [0.037986259907484055, 7.943107789287751e-07]
net.attn_layers.layers.6.1.to_v.weight [0.039473552256822586, 5.270119345368585e-06]
net.attn_layers.layers.6.1.to_out.weight [0.042068932205438614, -2.5879002350848168e-05]
net.attn_layers.layers.7.1.ff.0.0.weight [0.04763361066579819, -4.2162984755123034e-05]
net.attn_layers.layers.7.1.ff.2.weight [0.038815803825855255, 8.048945346672554e-06]
net.attn_layers.layers.8.1.to_q.weight [0.04069686681032181, 2.0148807379882783e-05]
net.attn_layers.layers.8.1.to_k.weight [0.04020101577043533, 3.761302286875434e-05]
net.attn_layers.layers.8.1.to_v.weight [0.04008425772190094, -7.266022294061258e-05]
net.attn_layers.layers.8.1.to_out.weight [0.04270505532622337, -6.767716695321724e-05]
net.attn_layers.layers.9.1.ff.0.0.weight [0.04809753969311714, -6.726665742462501e-05]
net.attn_layers.layers.9.1.ff.2.weight [0.040497973561286926, 3.222777422706713e-06]
net.attn_layers.layers.10.1.to_q.weight [0.04337010532617569, -2.8679363822448067e-05]
net.attn_layers.layers.10.1.to_k.weight [0.042823467403650284, -7.775587437208742e-05]
net.attn_layers.layers.10.1.to_v.weight [0.04079297557473183, 1.332701685896609e-05]
net.attn_layers.layers.10.1.to_out.weight [0.04710805416107178, -1.143753070209641e-05]
net.attn_layers.layers.11.1.ff.0.0.weight [0.04818108305335045, -6.860635039629415e-05]
net.attn_layers.layers.11.1.ff.2.weight [0.042630020529031754, -8.211573003791273e-06]
net.to_logits.weight [0.07387863099575043, -0.0001549593871459365]

Run history:

loss	█▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
lr	▁▂▃▃▄▅▆▆▇████▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁
time	▁▄▄▄▄▆▆▇▆▆█▆▆▆█▆▆▅▇▇▆▆█▆▆▆▆▆█▆▇▇▇▅▇▇▇▆▆▆

Run summary:

loss	0.53915
lr	0.0
time	276188054

View run x-transformers at: https://wandb.ai/team-ad8e/tinystories2/runs/qorimmsn
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Find logs at: ./wandb/run-20240105_233911-qorimmsn/logs
